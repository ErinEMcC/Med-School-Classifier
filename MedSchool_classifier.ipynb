{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier for Med School Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Classification\n",
    "How would you describe apples to a computer? How do they differ from oranges?\n",
    "Remember, computers can only really understand numbers, true false values, and strings within a predefined set.\n",
    "For example - you have shared data on fruit to your machine. An apple has a height feature, width feature, color feature, etc... After \"learning\" all the fruits, when the machine comes across an unknown, \"unlabeled\" fruit, it can use its previous experience (the data you shared) to **Classify** the object into a label / class (in the example below, Orange) based on that object's known **Features**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USEFUL TERMS\n",
    "**FEATURES**: Properties that describe data attributes for machine learning - often the variables <br>\n",
    "**FEATURE VECTOR aka FEATURE REPRESENTATION**: A set of features for a particular item of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fruit_example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Source: Andrew Rosenberg_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WE** are going to classify two different sets of documents a corpus of Med School Applications:  \n",
    "1. Students who entered the Family Medicine specialization\n",
    "2. Students who chose a different specialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, we can't use the same features as the fruit example - a digital text document won't have color or weight etc. and we want to use something more _meaningful_.  \n",
    "\n",
    "One method is to use a **Bags of Words** feature representation. In a Bag-of-words model, a text (e.g. an individual application document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity.\n",
    "\n",
    "So the frequency/ccurrence of each word will be used as the document's feature for training our classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BAG of WORDS EXAMPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"BoW_example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Source: Wikipedia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The TO-DO List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **scikit-learn** tools (along with a few other useful tools) we're going to analyze our dataset of text documents (i.e. all the Med School Applications):\n",
    "1. Load our Application data (texts) and organize it according to the categories (Fam Med / Other)\n",
    "2. Extract feature vectors suitable for machine learning _aka_ Build the Bag-of-Words model for each document\n",
    "3. Train & Test classifier(s) to perform categorization\n",
    "4. Finetuning the Classifier / Use a grid search strategy to find a good configuration of both the feature extraction components and the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LOADING & ORGANIZING TEXT DATA \n",
    "We need to first load our JSON datafile (text and metadata from med school applications) & split it into the two categories by the True/False metadata point in our JSON dictionary. Then we'll check the count (should be Total: 260)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Docs: 260\n",
      "Fam Med Docs: 135\n",
      "Other Docs: 125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "#create lists to contain JUST the text part for each group (fm/non) of docs \n",
    "fm_texts= []\n",
    "other_texts= []\n",
    "\n",
    "#open/read our data then split and append into the lists above\n",
    "with open('familyMedicine_031720.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    for doc in data:\n",
    "        if doc['keyword'] == 'TRUE':\n",
    "            true_text = doc['text']\n",
    "            fm_texts.append(true_text)\n",
    "        if doc['keyword'] == 'FALSE':\n",
    "            false_text = doc['text']\n",
    "            other_texts.append(false_text)\n",
    "            \n",
    "#check our count to make sure we've got things right\n",
    "print('Total Docs:', len(data))\n",
    "print('Fam Med Docs:', len(fm_texts))\n",
    "print('Other Docs:', len(other_texts))\n",
    "print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help us organize a bit more, I like to use the PANDAS library. PANDAS DataFrames are essentially spreadsheets (rows columns). Having our dataset in this format will help us check in on our data, explore a bit, and cut all the data we don't need to include.  We want every row to be a doc with just the row number, text, and classification. We could skip this step and use data straight from our JSON in the following steps, but for me this just helps me orient a bit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text   label\n",
      "0  I am a DAP (Dual Admissions Program) Student. ...  family\n",
      "1  I am a Dual Admissions Student. Student Worker...  family\n",
      "2  Throughout my life it has been proven to me ti...  family\n",
      "3  When I was in eighth grade, I had a personal m...  family\n",
      "4  I am ten. We're at Grandma's for the holidays....  family\n",
      "\n",
      "                                                text  label\n",
      "0  We each live once.  To live life to its fulles...  other\n",
      "1  Drums and feet pounded in Lubwe, a town in rur...  other\n",
      "2  Children, as patients, are almost never found ...  other\n",
      "3  My father used to tell me Learn from your past...  other\n",
      "4  In September of 2005 my friend Mark learned th...  other\n",
      "\n",
      "family    135\n",
      "other     125\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fmdf = pd.DataFrame({'text': fm_texts,\n",
    "                    'label':'family'})\n",
    "othdf = pd.DataFrame({'text':other_texts, \n",
    "                    'label':'other'})\n",
    "\n",
    "# combining our two dataframes into 1\n",
    "df = pd.concat([fmdf, othdf])\n",
    "\n",
    "print (fmdf.head())\n",
    "print ()\n",
    "print (othdf.head())\n",
    "print ()\n",
    "print (df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extracting features from text files _aka_ Build the Bag-of-Words\n",
    "Before we can perform any machine learning tasks on text documents, we need to make our text content numerical feature vectors. Remember, computers can only really understand numbers, true/false values, and strings within a predefined set.\n",
    "\n",
    "**Bags of Words**\n",
    "- Assign a fixed integer id to each word occurring in any document of the training set (by building a dictionary from words to integer indices). \n",
    "- For each document #i, count the number of occurrences of each word w and store it in X[i, j] as the value of feature #j where j is the index of word w in the dictionary.\n",
    "\n",
    "**E.G. 'family' is indexed at 7015 and any document in our dataset that uses 'student' will list 7015 and then the number of times that document uses 'student'.**\n",
    "\n",
    "NOTE: The bags of words representation implies that n_features is the number of distinct words in the corpus: this number is typically larger than 100,000. If n_samples == 10000, storing X as a NumPy array of type float32 would require 10000 x 100000 x 4 bytes = 4GB in RAM which is barely manageable on today’s computers. Fortunately, most values in X will be zeros since for a given document less than a few thousand distinct words will be used. For this reason we say that bags of words are typically high-dimensional sparse datasets. We can save a lot of memory by only storing the non-zero parts of the feature vectors in memory. scipy.sparse matrices are data structures that do exactly this, and scikit-learn has built-in support for these structures. (_source: scikit-learn tutorial, Working with Text Data_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing & Counting Text\n",
    "We know we need to count the occurences of each term in each doc, but we also need to do some text pre-processing tasks like:<br>\n",
    "- _Tokenizing_: breaking docs into words<br>\n",
    "- _Filtering Stopwords_: removing less meaningful words like articles and prepositions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these tasks are already included in SciKit-Learn's CountVectorizer tool. So we can accomplish these AND build a dictionary of features (all the unique words in the total dataset) AND transform individual documents into feature vectors - all in one fairly small chunk of code.\n",
    "<br>\n",
    "<br>\n",
    "**NOTE: at this point is where you might want to apply [TFIDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), but we are going to skip narrowing our terms down by frequency since we are dealing with a fairly small dataset (260 documents).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(260, 20234)\n",
      "  (0, 4877)\t3\n",
      "  (0, 5904)\t3\n",
      "  (0, 727)\t3\n",
      "  (0, 14348)\t3\n",
      "  (0, 17526)\t2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(stop_words='english')\n",
    "fv = count_vect.fit_transform(df['text'])\n",
    "print(fv.shape)\n",
    "print(fv[:1])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once fitted, the vectorizer has built a dictionary of feature indices.<br>\n",
    "The index value of a word in the vocabulary is linked to its frequency in the whole training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7015\n"
     ]
    }
   ],
   "source": [
    "print(count_vect.vocabulary_.get(u'family'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF: let's realllllly think about what frequency means ay\n",
    "Longer documents will have higher avg counts than shorter documents, even though they might talk about the same topics.\n",
    "\n",
    "To avoid discrepancies, we can divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called tf for Term Frequencies.\n",
    "\n",
    "Another refinement on top of tf is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n",
    "\n",
    "This downscaling is called **tf–idf** for “Term Frequency times Inverse Document Frequency”.\n",
    "\n",
    "Both tf and tf–idf can be computed as follows using TfidfTransformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(260, 20234)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(fv)\n",
    "fv_tf = tf_transformer.transform(fv)\n",
    "print(fv_tf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training a Classifier\n",
    "### Partitioning data into train and test sets\n",
    "When partitioning data into train and test sets, a good place to start is to use 75% of your data for training, and 25% of your data for testing. We want as much training data as possible so the machine/algorithm can \"learn\" (imagine telling a baby 'this is a lemon' 1 time versus 100 times), while also having enough testing data to ensure that our trained classifier is generalizable across a number of examples. This will also lead to more accurate evalutation of our trained classifier.\n",
    "\n",
    "Again, scikit-learn has a function that will do exactly this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(fv, df['label'],\n",
    "                                                stratify=df['label'], \n",
    "                                                test_size=0.25,\n",
    "                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We use the \"stratify\" argument because we have an uneven amount of training data; we have more Family Medicine Applications (135) than Other Applications (125). By using stratify, we ensure that our classifier will take this data imbalence into account.\n",
    "\n",
    "* In this example, we are using a fixed random state, to ensure we will always get exactly the same value when we classify. Adding this argument is unnecessary for most types of classification; we do it here to ensure our results do not vary slightly across runs. <br>**Fun Fact Alert!!** 42 is commonly used for the fixed random state b/c of _Hitchhiker's Guide to the Galaxy_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHECK-IN: How many docs do we have now in each of our Train / Test groups? And what is the total unique vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(195, 20234)\n",
      "(65, 20234)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"algorithm_map.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Source: Andreas Mueller*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**START** <br>\n",
    "- More than 50 samples: YES (260 docs)<br>\n",
    "- Predicting a category: YES!<br>\n",
    "- Have labeled data: YES!<br>\n",
    "- Less than 100k samples: YES (Again, 260 docs)\n",
    "\n",
    "**END: Linear SVC Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.A Trying the Linear SVC Classifier\n",
    "We're actually going to try a few different classifiers - but let's start with the **Linear SVC Classifier**<br>\n",
    "We import that Classifier from scikit-learn, then \"FIT\" our \"Trainging\" data to it (75% of our total dataset).<br> This gives the machine something to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier_svc = SVC().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the Classifier's predict abilities on the \"Testing\" data (25% of our total dataset) and see what percentage it correctly guesses by comparing the machine's predictions to the testing data's *actual* classification (family/other)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "predicted_svc = classifier.predict(X_test)\n",
    "np.mean(predicted_svc == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see what that 60% score looks like across our classifications using a Confusion Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19 15]\n",
      " [11 20]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print (confusion_matrix(y_test, predicted_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"confusionMatrix_svc.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.B Trying a New Classifer!\n",
    "**Naïve Bayes** (probabilistic) classifier.<br>\n",
    "<br>_A probabilistic classifier is able to predict, given an observation of an input (our dataset's feature representation), a probability distribution over a set of classes, rather than only outputting the most likely class that the observation should belong to._ ([Wikipedia, Probabilistic classification](https://en.wikipedia.org/wiki/Probabilistic_classification))<br>\n",
    "<br>\n",
    "Scikit-learn includes several variants of Naïve Bayes classifier - we'll use the one most suitable for word counts:<br> \n",
    "**The Multinomial Variant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier_mnb = MultinomialNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs_new = ['a personal medical experience ignited my passion to pursue a career in medicine.', 'I am a Dual admissions program student']\n",
    "#X_new_counts = count_vect.transform(docs_new)\n",
    "\n",
    "predicted_mnb = classifier_mnb.predict(X_test)\n",
    "\n",
    "#for doc, label in zip(X_test, predicted):\n",
    " #   print('%r => %s' % (doc, label))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_mnb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[28  6]\n",
      " [20 11]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print (confusion_matrix(y_test, predicted_mnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"confusion_matrix.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok so decidely not a great score here. SO FAR ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a ton of highly successful classifiers, ay? Well, that's where finetuning a Classifier comes in and that's where I'll start applying some of the hand-coded categories of Jackie's **extremly thorough** tagging of W2V outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINETUNING a CLASSIFIER using hand-tagged categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur feugiat, eros a tristique aliquam, leo augue hendrerit neque, pulvinar suscipit urna ex ac dolor. Maecenas metus ligula, tempor id ante non, fringilla euismod massa. Nam tempus congue justo, eu maximus nibh pellentesque vel. Phasellus commodo sapien in velit finibus sollicitudin. Pellentesque semper et nibh at faucibus. Mauris dignissim tincidunt metus, cursus malesuada libero rhoncus varius. Vivamus nisi neque, hendrerit et nisi quis, malesuada tincidunt est. Praesent malesuada imperdiet ultrices. In hac habitasse platea dictumst. Donec risus enim, hendrerit vel tincidunt in, sollicitudin accumsan metus.* <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHOUTOUTS!\n",
    "Jackie Knapke! and everyone else who contributed from the:<br>\n",
    "Dept of Family & Community Medicine - University of Cincinnati, College of Medicine<br>\n",
    "[SciKit-Learn \"Working With Text Data\" Tutorial](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) <br>\n",
    "[CUNY DHRI Open Curricula: Intro to Machine Learning](https://www.dhinstitutes.org/curricula/)<br>\n",
    "[Finetuning a Classifier in SciKit-Learn](https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
